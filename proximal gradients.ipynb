{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data1 = np.array([[0,1,0],[0.5,0.5,1],[1,0,0]])\n",
    "y_data1= np.array([1,0.75,0.5])\n",
    "beta1 = np.array([0.0,0.0,0.0])\n",
    "\n",
    "class SPARSA :\n",
    "    def __init__(self,):\n",
    "        self.alpha = 1\n",
    "        self.lam_current = np.inf\n",
    "        self.lamda = 0.1\n",
    "        self.s_f_lam = 0.5\n",
    "\n",
    "        \n",
    "   \n",
    "    def gradient_loss(self,x_data,y_data,beta):\n",
    "        y_hat = np.dot(x_data,beta)\n",
    "        loss = 0.5 * np.sum((y_hat - y_data)**2)\n",
    "        grad = np.dot(x_data.transpose(),(y_hat - y_data))      \n",
    "        return grad\n",
    "    \n",
    "  \n",
    "    def soft_threshold(self,x_n,lamda):\n",
    "        first = x_n - (lamda)\n",
    "        second =first * np.sign(x_n)\n",
    "        third = np.abs(x_n)> lamda\n",
    "        soft = second * third\n",
    "        return soft\n",
    "    \n",
    "    def brazilai_step(self,bet_old,bet_new,x_data,y_data):\n",
    "        diff = self.gradient_loss(x_data,y_data,bet_new) - self.gradient_loss(x_data,y_data,bet_old)\n",
    "        first = np.dot((bet_new - bet_old),diff)\n",
    "        bottom = np.dot((bet_new-bet_old),(bet_new - bet_old).transpose())\n",
    "        alph = first/bottom\n",
    "        return alph\n",
    "    \n",
    "   \n",
    "    def aceptance_criterion(self,beta, x_data, y_data,lamda):\n",
    "        y_hat = np.dot(x_data,beta.transpose())\n",
    "        loss = 0.5 * np.sum((y_data-y_hat)**2)\n",
    "        criterion = lamda * np.linalg.norm(beta,ord = 1)\n",
    "        return criterion\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self,x_data,y_data,beta):\n",
    "        lamda = self.lamda\n",
    "        alpha = self.alpha\n",
    "        lam_current = self.lam_current\n",
    "        s = self.s_f_lam \n",
    " \n",
    "        while lam_current != lamda:\n",
    "            lam_current = max(s * np.linalg.norm(self.gradient_loss(x_data,y_data,beta),ord = np.inf),lamda)\n",
    "            \n",
    "            print('\"using the lamda_current :\"',lam_current)\n",
    "            while True:\n",
    "                beta_old = beta\n",
    "                gradient = self.gradient_loss(x_data,y_data,beta)\n",
    "                u = beta - (gradient/alpha).transpose()\n",
    "                beta = self.soft_threshold(u,lam_current/alpha)\n",
    "                alpha = self.brazilai_step(beta_old,beta,x_data,y_data)\n",
    "                if self.aceptance_criterion(beta, x_data, y_data,lamda) <self.aceptance_criterion(beta_old, x_data, y_data,lamda):\n",
    "                    break\n",
    "                print(\"beta {}\".format(beta))\n",
    "                print(\"gradient {}\".format(gradient))\n",
    "                print(\"u {}\".format(u))\n",
    "                print(\"prediction {}\".format(np.dot(x_data,beta.transpose())))\n",
    "                print(\"alpha {}\".format(alpha))\n",
    "                print(\"function {}\\n\".format(self.aceptance_criterion(beta, x_data, y_data,lamda)))\n",
    "                print(\"next loop\\n\")\n",
    "            print(\"\\n\\n\")\n",
    "        print(\"optimization finished\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"using the lamda_current :\" 0.6875\n",
      "beta [0.1875 0.6875 0.0625]\n",
      "gradient [-0.875 -1.375 -0.75 ]\n",
      "u [0.875 1.375 0.75 ]\n",
      "prediction [0.6875 0.5    0.1875]\n",
      "alpha 1.4809160305343512\n",
      "function 0.09375\n",
      "\n",
      "next loop\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"using the lamda_current :\" 0.3609858247422681\n",
      "beta [0.21764013 0.71764013 0.06631819]\n",
      "gradient [-0.72197165 -0.72197165 -0.48131443]\n",
      "u [0.4165947  0.9165947  0.26527275]\n",
      "prediction [0.71764013 0.53395832 0.21764013]\n",
      "alpha 1.7894736842105263\n",
      "function 0.10015984516590555\n",
      "\n",
      "next loop\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"using the lamda_current :\" 0.19944998677738135\n",
      "beta [0.55922011 1.05922011 0.10838447]\n",
      "gradient [-0.39889997 -0.39889997 -0.26593332]\n",
      "u [0.88437353 1.38437353 0.4335379 ]\n",
      "prediction [1.05922011 0.91760458 0.55922011]\n",
      "alpha 1.7894736842105263\n",
      "function 0.17268246934791365\n",
      "\n",
      "next loop\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"using the lamda_current :\" 0.1\n",
      "beta [0.41903408 0.91903408 0.01675987]\n",
      "gradient [-0.19824228 -0.19824228 -0.13216152]\n",
      "u [0.47114565 0.97114565 0.06887145]\n",
      "prediction [0.91903408 0.68579395 0.41903408]\n",
      "alpha 1.7852894087932352\n",
      "function 0.1354828031090113\n",
      "\n",
      "next loop\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    "b = SPARSA()\n",
    "c = b.train(x_data1,y_data1,beta1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
